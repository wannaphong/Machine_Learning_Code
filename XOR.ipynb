{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR In PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## เตรียมข้อมูล\n",
    "\n",
    "เตรียมข้อมูลสำหรับใช้ train XOR โดยเราจะต้องนำข้อมูลไปผ่านให้กลายเป็น Tensor แล้วใช้ Variable เพื่อให้สามารถนำค่าไปใช้งานได้แล้วเก็บรวมไว้ใน list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "X = list(map(lambda s: Variable(torch.Tensor([s])).cpu(), [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "]))\n",
    "y = list(map(lambda s: Variable(torch.Tensor([s])).cpu(), [\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "]))\n",
    "#X = torch.from_numpy(np.array([[0,0],[0,1],[1,0],[1,1]]))\n",
    "#y = torch.from_numpy(np.array([[0],[1],[1],[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เติม .cuda() เพื่อใช้ GPU ถ้าเติม .cpu() จะใช้ CPU โดย model ต้องเลือกให้สัมพันธ์กับข้อมูลว่าเป็น GPU หรือ CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 0.]]),\n",
       " tensor([[0., 1.]]),\n",
       " tensor([[1., 0.]]),\n",
       " tensor([[1., 1.]])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Variable(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.]]), tensor([[1.]]), tensor([[1.]]), tensor([[0.]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_num_units=X[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_num_units=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_num_units=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(input_num_units, output_num_units)\n",
    "b = torch.randn(hidden_num_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    " torch.nn.Linear(input_num_units, hidden_num_units),\n",
    " torch.nn.Sigmoid(),\n",
    " torch.nn.Linear(hidden_num_units, output_num_units),\n",
    ").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=8, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(input_num_units, hidden_num_units)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(hidden_num_units, output_num_units)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer with sigmoid activation\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        # Output layer with softmax activation\n",
    "        x = self.output(x)#F.softmax(self.output(x))\n",
    "        \n",
    "        return x'''\n",
    "#net=model().cuda()\n",
    "loss_fn = nn.MSELoss()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Sequential.forward of Sequential(\n",
       "  (0): Linear(in_features=2, out_features=8, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=8, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch        1 Loss: 70.6503689289093%\n",
      "Epoch        2 Loss: 79.41150665283203%\n",
      "Epoch        3 Loss: 78.05562019348145%\n",
      "Epoch        4 Loss: 76.57843828201294%\n",
      "Epoch        5 Loss: 75.15897750854492%\n",
      "Epoch        6 Loss: 73.79730939865112%\n",
      "Epoch        7 Loss: 72.49055504798889%\n",
      "Epoch        8 Loss: 71.23610973358154%\n",
      "Epoch        9 Loss: 70.03152370452881%\n",
      "Epoch       10 Loss: 68.874591588974%\n",
      "Epoch       11 Loss: 67.76334643363953%\n",
      "Epoch       12 Loss: 66.69585704803467%\n",
      "Epoch       13 Loss: 65.67037105560303%\n",
      "Epoch       14 Loss: 64.68527913093567%\n",
      "Epoch       15 Loss: 63.73901963233948%\n",
      "Epoch       16 Loss: 62.8301203250885%\n",
      "Epoch       17 Loss: 61.95717453956604%\n",
      "Epoch       18 Loss: 61.118894815444946%\n",
      "Epoch       19 Loss: 60.31391620635986%\n",
      "Epoch       20 Loss: 59.54107046127319%\n",
      "Epoch       21 Loss: 58.79910588264465%\n",
      "Epoch       22 Loss: 58.08690786361694%\n",
      "Epoch       23 Loss: 57.40333795547485%\n",
      "Epoch       24 Loss: 56.74728751182556%\n",
      "Epoch       25 Loss: 56.117719411849976%\n",
      "Epoch       26 Loss: 55.51358461380005%\n",
      "Epoch       27 Loss: 54.9339234828949%\n",
      "Epoch       28 Loss: 54.377710819244385%\n",
      "Epoch       29 Loss: 53.8440465927124%\n",
      "Epoch       30 Loss: 53.331971168518066%\n",
      "Epoch       31 Loss: 52.840644121170044%\n",
      "Epoch       32 Loss: 52.369171380996704%\n",
      "Epoch       33 Loss: 51.91674828529358%\n",
      "Epoch       34 Loss: 51.482534408569336%\n",
      "Epoch       35 Loss: 51.06577277183533%\n",
      "Epoch       36 Loss: 50.665682554244995%\n",
      "Epoch       37 Loss: 50.281572341918945%\n",
      "Epoch       38 Loss: 49.91273283958435%\n",
      "Epoch       39 Loss: 49.55846071243286%\n",
      "Epoch       40 Loss: 49.21813607215881%\n",
      "Epoch       41 Loss: 48.891136050224304%\n",
      "Epoch       42 Loss: 48.57683181762695%\n",
      "Epoch       43 Loss: 48.27468991279602%\n",
      "Epoch       44 Loss: 47.984108328819275%\n",
      "Epoch       45 Loss: 47.70459830760956%\n",
      "Epoch       46 Loss: 47.435641288757324%\n",
      "Epoch       47 Loss: 47.17673659324646%\n",
      "Epoch       48 Loss: 46.927449107170105%\n",
      "Epoch       49 Loss: 46.687301993370056%\n",
      "Epoch       50 Loss: 46.45591080188751%\n",
      "Epoch       51 Loss: 46.23282849788666%\n",
      "Epoch       52 Loss: 46.01769745349884%\n",
      "Epoch       53 Loss: 45.81015110015869%\n",
      "Epoch       54 Loss: 45.60980498790741%\n",
      "Epoch       55 Loss: 45.416346192359924%\n",
      "Epoch       56 Loss: 45.22946774959564%\n",
      "Epoch       57 Loss: 45.04883587360382%\n",
      "Epoch       58 Loss: 44.87418830394745%\n",
      "Epoch       59 Loss: 44.70522105693817%\n",
      "Epoch       60 Loss: 44.54168677330017%\n",
      "Epoch       61 Loss: 44.38334405422211%\n",
      "Epoch       62 Loss: 44.22993063926697%\n",
      "Epoch       63 Loss: 44.08122897148132%\n",
      "Epoch       64 Loss: 43.93703043460846%\n",
      "Epoch       65 Loss: 43.79710853099823%\n",
      "Epoch       66 Loss: 43.661296367645264%\n",
      "Epoch       67 Loss: 43.529364466667175%\n",
      "Epoch       68 Loss: 43.40117871761322%\n",
      "Epoch       69 Loss: 43.27652454376221%\n",
      "Epoch       70 Loss: 43.155306577682495%\n",
      "Epoch       71 Loss: 43.03729236125946%\n",
      "Epoch       72 Loss: 42.92238652706146%\n",
      "Epoch       73 Loss: 42.81044006347656%\n",
      "Epoch       74 Loss: 42.70130395889282%\n",
      "Epoch       75 Loss: 42.594873905181885%\n",
      "Epoch       76 Loss: 42.49100387096405%\n",
      "Epoch       77 Loss: 42.38961040973663%\n",
      "Epoch       78 Loss: 42.290547490119934%\n",
      "Epoch       79 Loss: 42.193737626075745%\n",
      "Epoch       80 Loss: 42.09906458854675%\n",
      "Epoch       81 Loss: 42.006438970565796%\n",
      "Epoch       82 Loss: 41.91577434539795%\n",
      "Epoch       83 Loss: 41.82696342468262%\n",
      "Epoch       84 Loss: 41.7399525642395%\n",
      "Epoch       85 Loss: 41.65463745594025%\n",
      "Epoch       86 Loss: 41.57094657421112%\n",
      "Epoch       87 Loss: 41.48882329463959%\n",
      "Epoch       88 Loss: 41.408178210258484%\n",
      "Epoch       89 Loss: 41.32894277572632%\n",
      "Epoch       90 Loss: 41.25107824802399%\n",
      "Epoch       91 Loss: 41.174495220184326%\n",
      "Epoch       92 Loss: 41.09915494918823%\n",
      "Epoch       93 Loss: 41.0249799489975%\n",
      "Epoch       94 Loss: 40.95192551612854%\n",
      "Epoch       95 Loss: 40.879952907562256%\n",
      "Epoch       96 Loss: 40.808990597724915%\n",
      "Epoch       97 Loss: 40.73899984359741%\n",
      "Epoch       98 Loss: 40.66993296146393%\n",
      "Epoch       99 Loss: 40.60174524784088%\n",
      "Epoch      100 Loss: 40.53439795970917%\n",
      "Epoch      101 Loss: 40.467849373817444%\n",
      "Epoch      102 Loss: 40.40205180644989%\n",
      "Epoch      103 Loss: 40.33697247505188%\n",
      "Epoch      104 Loss: 40.27255475521088%\n",
      "Epoch      105 Loss: 40.20880460739136%\n",
      "Epoch      106 Loss: 40.14565050601959%\n",
      "Epoch      107 Loss: 40.083059668540955%\n",
      "Epoch      108 Loss: 40.02102315425873%\n",
      "Epoch      109 Loss: 39.959484338760376%\n",
      "Epoch      110 Loss: 39.898425340652466%\n",
      "Epoch      111 Loss: 39.83781635761261%\n",
      "Epoch      112 Loss: 39.777615666389465%\n",
      "Epoch      113 Loss: 39.717814326286316%\n",
      "Epoch      114 Loss: 39.65837359428406%\n",
      "Epoch      115 Loss: 39.59925174713135%\n",
      "Epoch      116 Loss: 39.540454745292664%\n",
      "Epoch      117 Loss: 39.481931924819946%\n",
      "Epoch      118 Loss: 39.42367136478424%\n",
      "Epoch      119 Loss: 39.36562538146973%\n",
      "Epoch      120 Loss: 39.30779993534088%\n",
      "Epoch      121 Loss: 39.25015032291412%\n",
      "Epoch      122 Loss: 39.1926646232605%\n",
      "Epoch      123 Loss: 39.13532495498657%\n",
      "Epoch      124 Loss: 39.078089594841%\n",
      "Epoch      125 Loss: 39.0209436416626%\n",
      "Epoch      126 Loss: 38.963863253593445%\n",
      "Epoch      127 Loss: 38.90684247016907%\n",
      "Epoch      128 Loss: 38.84985148906708%\n",
      "Epoch      129 Loss: 38.79286050796509%\n",
      "Epoch      130 Loss: 38.73586058616638%\n",
      "Epoch      131 Loss: 38.67881894111633%\n",
      "Epoch      132 Loss: 38.621729612350464%\n",
      "Epoch      133 Loss: 38.56455087661743%\n",
      "Epoch      134 Loss: 38.50730359554291%\n",
      "Epoch      135 Loss: 38.4499192237854%\n",
      "Epoch      136 Loss: 38.39241862297058%\n",
      "Epoch      137 Loss: 38.33475112915039%\n",
      "Epoch      138 Loss: 38.27692270278931%\n",
      "Epoch      139 Loss: 38.2189005613327%\n",
      "Epoch      140 Loss: 38.16065788269043%\n",
      "Epoch      141 Loss: 38.10218870639801%\n",
      "Epoch      142 Loss: 38.04348409175873%\n",
      "Epoch      143 Loss: 37.984499335289%\n",
      "Epoch      144 Loss: 37.925225496292114%\n",
      "Epoch      145 Loss: 37.865665555000305%\n",
      "Epoch      146 Loss: 37.805771827697754%\n",
      "Epoch      147 Loss: 37.745532393455505%\n",
      "Epoch      148 Loss: 37.68494427204132%\n",
      "Epoch      149 Loss: 37.62396574020386%\n",
      "Epoch      150 Loss: 37.56259083747864%\n",
      "Epoch      151 Loss: 37.50080466270447%\n",
      "Epoch      152 Loss: 37.43857741355896%\n",
      "Epoch      153 Loss: 37.375909090042114%\n",
      "Epoch      154 Loss: 37.312763929367065%\n",
      "Epoch      155 Loss: 37.24912106990814%\n",
      "Epoch      156 Loss: 37.184977531433105%\n",
      "Epoch      157 Loss: 37.12030053138733%\n",
      "Epoch      158 Loss: 37.05507814884186%\n",
      "Epoch      159 Loss: 36.98928952217102%\n",
      "Epoch      160 Loss: 36.922916769981384%\n",
      "Epoch      161 Loss: 36.855947971343994%\n",
      "Epoch      162 Loss: 36.788350343704224%\n",
      "Epoch      163 Loss: 36.72011196613312%\n",
      "Epoch      164 Loss: 36.651214957237244%\n",
      "Epoch      165 Loss: 36.58164143562317%\n",
      "Epoch      166 Loss: 36.511361598968506%\n",
      "Epoch      167 Loss: 36.44036650657654%\n",
      "Epoch      168 Loss: 36.36862635612488%\n",
      "Epoch      169 Loss: 36.29613816738129%\n",
      "Epoch      170 Loss: 36.22286319732666%\n",
      "Epoch      171 Loss: 36.14879250526428%\n",
      "Epoch      172 Loss: 36.07390522956848%\n",
      "Epoch      173 Loss: 35.99817156791687%\n",
      "Epoch      174 Loss: 35.92158257961273%\n",
      "Epoch      175 Loss: 35.844117403030396%\n",
      "Epoch      176 Loss: 35.76574623584747%\n",
      "Epoch      177 Loss: 35.68644821643829%\n",
      "Epoch      178 Loss: 35.60620844364166%\n",
      "Epoch      179 Loss: 35.524991154670715%\n",
      "Epoch      180 Loss: 35.44280230998993%\n",
      "Epoch      181 Loss: 35.359591245651245%\n",
      "Epoch      182 Loss: 35.27533411979675%\n",
      "Epoch      183 Loss: 35.19003391265869%\n",
      "Epoch      184 Loss: 35.103657841682434%\n",
      "Epoch      185 Loss: 35.01616716384888%\n",
      "Epoch      186 Loss: 34.927546977996826%\n",
      "Epoch      187 Loss: 34.83777046203613%\n",
      "Epoch      188 Loss: 34.746816754341125%\n",
      "Epoch      189 Loss: 34.65465605258942%\n",
      "Epoch      190 Loss: 34.56125855445862%\n",
      "Epoch      191 Loss: 34.4666063785553%\n",
      "Epoch      192 Loss: 34.370654821395874%\n",
      "Epoch      193 Loss: 34.27340388298035%\n",
      "Epoch      194 Loss: 34.17479991912842%\n",
      "Epoch      195 Loss: 34.07482206821442%\n",
      "Epoch      196 Loss: 33.97343158721924%\n",
      "Epoch      197 Loss: 33.87060463428497%\n",
      "Epoch      198 Loss: 33.76632034778595%\n",
      "Epoch      199 Loss: 33.660516142845154%\n",
      "Epoch      200 Loss: 33.553192019462585%\n",
      "Epoch      201 Loss: 33.44428241252899%\n",
      "Epoch      202 Loss: 33.3337664604187%\n",
      "Epoch      203 Loss: 33.22161138057709%\n",
      "Epoch      204 Loss: 33.10777544975281%\n",
      "Epoch      205 Loss: 32.99221396446228%\n",
      "Epoch      206 Loss: 32.8748881816864%\n",
      "Epoch      207 Loss: 32.75575339794159%\n",
      "Epoch      208 Loss: 32.63477683067322%\n",
      "Epoch      209 Loss: 32.51190185546875%\n",
      "Epoch      210 Loss: 32.38709568977356%\n",
      "Epoch      211 Loss: 32.260310649871826%\n",
      "Epoch      212 Loss: 32.1314811706543%\n",
      "Epoch      213 Loss: 32.00056850910187%\n",
      "Epoch      214 Loss: 31.86752200126648%\n",
      "Epoch      215 Loss: 31.73230290412903%\n",
      "Epoch      216 Loss: 31.594830751419067%\n",
      "Epoch      217 Loss: 31.45504891872406%\n",
      "Epoch      218 Loss: 31.31292164325714%\n",
      "Epoch      219 Loss: 31.16837441921234%\n",
      "Epoch      220 Loss: 31.021371483802795%\n",
      "Epoch      221 Loss: 30.871817469596863%\n",
      "Epoch      222 Loss: 30.7196706533432%\n",
      "Epoch      223 Loss: 30.5648535490036%\n",
      "Epoch      224 Loss: 30.407312512397766%\n",
      "Epoch      225 Loss: 30.24696707725525%\n",
      "Epoch      226 Loss: 30.083748698234558%\n",
      "Epoch      227 Loss: 29.917606711387634%\n",
      "Epoch      228 Loss: 29.74843978881836%\n",
      "Epoch      229 Loss: 29.576218128204346%\n",
      "Epoch      230 Loss: 29.400822520256042%\n",
      "Epoch      231 Loss: 29.222199320793152%\n",
      "Epoch      232 Loss: 29.040294885635376%\n",
      "Epoch      233 Loss: 28.85499894618988%\n",
      "Epoch      234 Loss: 28.666257858276367%\n",
      "Epoch      235 Loss: 28.47399413585663%\n",
      "Epoch      236 Loss: 28.278130292892456%\n",
      "Epoch      237 Loss: 28.07859480381012%\n",
      "Epoch      238 Loss: 27.87531614303589%\n",
      "Epoch      239 Loss: 27.66822874546051%\n",
      "Epoch      240 Loss: 27.457255125045776%\n",
      "Epoch      241 Loss: 27.242323756217957%\n",
      "Epoch      242 Loss: 27.023375034332275%\n",
      "Epoch      243 Loss: 26.800325512886047%\n",
      "Epoch      244 Loss: 26.57316029071808%\n",
      "Epoch      245 Loss: 26.341772079467773%\n",
      "Epoch      246 Loss: 26.106134057044983%\n",
      "Epoch      247 Loss: 25.866195559501648%\n",
      "Epoch      248 Loss: 25.62190294265747%\n",
      "Epoch      249 Loss: 25.37323236465454%\n",
      "Epoch      250 Loss: 25.120127201080322%\n",
      "Epoch      251 Loss: 24.862582981586456%\n",
      "Epoch      252 Loss: 24.600569903850555%\n",
      "Epoch      253 Loss: 24.334073066711426%\n",
      "Epoch      254 Loss: 24.063095450401306%\n",
      "Epoch      255 Loss: 23.787622153759003%\n",
      "Epoch      256 Loss: 23.5076904296875%\n",
      "Epoch      257 Loss: 23.223304748535156%\n",
      "Epoch      258 Loss: 22.93449342250824%\n",
      "Epoch      259 Loss: 22.64130711555481%\n",
      "Epoch      260 Loss: 22.343796491622925%\n",
      "Epoch      261 Loss: 22.042012214660645%\n",
      "Epoch      262 Loss: 21.736058592796326%\n",
      "Epoch      263 Loss: 21.425989270210266%\n",
      "Epoch      264 Loss: 21.111904084682465%\n",
      "Epoch      265 Loss: 20.793934166431427%\n",
      "Epoch      266 Loss: 20.472176373004913%\n",
      "Epoch      267 Loss: 20.146796107292175%\n",
      "Epoch      268 Loss: 19.8179230093956%\n",
      "Epoch      269 Loss: 19.48571652173996%\n",
      "Epoch      270 Loss: 19.150342047214508%\n",
      "Epoch      271 Loss: 18.811993300914764%\n",
      "Epoch      272 Loss: 18.470880389213562%\n",
      "Epoch      273 Loss: 18.12720149755478%\n",
      "Epoch      274 Loss: 17.781156301498413%\n",
      "Epoch      275 Loss: 17.433008551597595%\n",
      "Epoch      276 Loss: 17.082983255386353%\n",
      "Epoch      277 Loss: 16.7313352227211%\n",
      "Epoch      278 Loss: 16.37832522392273%\n",
      "Epoch      279 Loss: 16.024228930473328%\n",
      "Epoch      280 Loss: 15.6693235039711%\n",
      "Epoch      281 Loss: 15.313901007175446%\n",
      "Epoch      282 Loss: 14.958232641220093%\n",
      "Epoch      283 Loss: 14.60263580083847%\n",
      "Epoch      284 Loss: 14.247393608093262%\n",
      "Epoch      285 Loss: 13.892826437950134%\n",
      "Epoch      286 Loss: 13.539235293865204%\n",
      "Epoch      287 Loss: 13.186930119991302%\n",
      "Epoch      288 Loss: 12.836211919784546%\n",
      "Epoch      289 Loss: 12.48740702867508%\n",
      "Epoch      290 Loss: 12.140800058841705%\n",
      "Epoch      291 Loss: 11.796695739030838%\n",
      "Epoch      292 Loss: 11.455392092466354%\n",
      "Epoch      293 Loss: 11.11719235777855%\n",
      "Epoch      294 Loss: 10.782358795404434%\n",
      "Epoch      295 Loss: 10.451193153858185%\n",
      "Epoch      296 Loss: 10.123953223228455%\n",
      "Epoch      297 Loss: 9.800883382558823%\n",
      "Epoch      298 Loss: 9.482262283563614%\n",
      "Epoch      299 Loss: 9.168300032615662%\n",
      "Epoch      300 Loss: 8.859237283468246%\n",
      "Epoch      301 Loss: 8.555282652378082%\n",
      "Epoch      302 Loss: 8.256635814905167%\n",
      "Epoch      303 Loss: 7.963484525680542%\n",
      "Epoch      304 Loss: 7.67599493265152%\n",
      "Epoch      305 Loss: 7.3943354189395905%\n",
      "Epoch      306 Loss: 7.118628919124603%\n",
      "Epoch      307 Loss: 6.849024444818497%\n",
      "Epoch      308 Loss: 6.58562034368515%\n",
      "Epoch      309 Loss: 6.328517943620682%\n",
      "Epoch      310 Loss: 6.0777876526117325%\n",
      "Epoch      311 Loss: 5.833503603935242%\n",
      "Epoch      312 Loss: 5.595722794532776%\n",
      "Epoch      313 Loss: 5.364475399255753%\n",
      "Epoch      314 Loss: 5.139774456620216%\n",
      "Epoch      315 Loss: 4.921649396419525%\n",
      "Epoch      316 Loss: 4.710076004266739%\n",
      "Epoch      317 Loss: 4.50504757463932%\n",
      "Epoch      318 Loss: 4.306520894169807%\n",
      "Epoch      319 Loss: 4.1144635528326035%\n",
      "Epoch      320 Loss: 3.928809240460396%\n",
      "Epoch      321 Loss: 3.749508038163185%\n",
      "Epoch      322 Loss: 3.576480969786644%\n",
      "Epoch      323 Loss: 3.4096360206604004%\n",
      "Epoch      324 Loss: 3.24888676404953%\n",
      "Epoch      325 Loss: 3.0941255390644073%\n",
      "Epoch      326 Loss: 2.9452523216605186%\n",
      "Epoch      327 Loss: 2.80215535312891%\n",
      "Epoch      328 Loss: 2.664704993367195%\n",
      "Epoch      329 Loss: 2.5327784940600395%\n",
      "Epoch      330 Loss: 2.406245470046997%\n",
      "Epoch      331 Loss: 2.2849760949611664%\n",
      "Epoch      332 Loss: 2.168823219835758%\n",
      "Epoch      333 Loss: 2.0576562732458115%\n",
      "Epoch      334 Loss: 1.9513286650180817%\n",
      "Epoch      335 Loss: 1.8496990203857422%\n",
      "Epoch      336 Loss: 1.7526201903820038%\n",
      "Epoch      337 Loss: 1.6599491238594055%\n",
      "Epoch      338 Loss: 1.5715394169092178%\n",
      "Epoch      339 Loss: 1.4872482046484947%\n",
      "Epoch      340 Loss: 1.4069320634007454%\n",
      "Epoch      341 Loss: 1.3304462656378746%\n",
      "Epoch      342 Loss: 1.2576482258737087%\n",
      "Epoch      343 Loss: 1.1884004808962345%\n",
      "Epoch      344 Loss: 1.122570876032114%\n",
      "Epoch      345 Loss: 1.0600155219435692%\n",
      "Epoch      346 Loss: 1.0006049647927284%\n",
      "Epoch      347 Loss: 0.9442146867513657%\n",
      "Epoch      348 Loss: 0.8907093666493893%\n",
      "Epoch      349 Loss: 0.8399753831326962%\n",
      "Epoch      350 Loss: 0.7918817922472954%\n",
      "Epoch      351 Loss: 0.7463217247277498%\n",
      "Epoch      352 Loss: 0.7031774148344994%\n",
      "Epoch      353 Loss: 0.6623353343456984%\n",
      "Epoch      354 Loss: 0.6236931774765253%\n",
      "Epoch      355 Loss: 0.5871464498341084%\n",
      "Epoch      356 Loss: 0.5525934975594282%\n",
      "Epoch      357 Loss: 0.5199398379772902%\n",
      "Epoch      358 Loss: 0.48909513279795647%\n",
      "Epoch      359 Loss: 0.459967739880085%\n",
      "Epoch      360 Loss: 0.43247248977422714%\n",
      "Epoch      361 Loss: 0.40652784518897533%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch      362 Loss: 0.3820533398538828%\n",
      "Epoch      363 Loss: 0.3589754458516836%\n",
      "Epoch      364 Loss: 0.3372172126546502%\n",
      "Epoch      365 Loss: 0.3167143324390054%\n",
      "Epoch      366 Loss: 0.29739802703261375%\n",
      "Epoch      367 Loss: 0.279204617254436%\n",
      "Epoch      368 Loss: 0.2620768966153264%\n",
      "Epoch      369 Loss: 0.2459527924656868%\n",
      "Epoch      370 Loss: 0.23078045342117548%\n",
      "Epoch      371 Loss: 0.2165063749998808%\n",
      "Epoch      372 Loss: 0.20308098755776882%\n",
      "Epoch      373 Loss: 0.19045681692659855%\n",
      "Epoch      374 Loss: 0.1785891712643206%\n",
      "Epoch      375 Loss: 0.1674340688623488%\n",
      "Epoch      376 Loss: 0.1569551764987409%\n",
      "Epoch      377 Loss: 0.14710872201249003%\n",
      "Epoch      378 Loss: 0.1378612476401031%\n",
      "Epoch      379 Loss: 0.12917793355882168%\n",
      "Epoch      380 Loss: 0.12102493783459067%\n",
      "Epoch      381 Loss: 0.11337216710671782%\n",
      "Epoch      382 Loss: 0.10619062231853604%\n",
      "Epoch      383 Loss: 0.09945144411176443%\n",
      "Epoch      384 Loss: 0.09313025511801243%\n",
      "Epoch      385 Loss: 0.08719955803826451%\n",
      "Epoch      386 Loss: 0.08163843303918839%\n",
      "Epoch      387 Loss: 0.07642291020601988%\n",
      "Epoch      388 Loss: 0.07153424085117877%\n",
      "Epoch      389 Loss: 0.06695157499052584%\n",
      "Epoch      390 Loss: 0.06265658885240555%\n",
      "Epoch      391 Loss: 0.05863119731657207%\n",
      "Epoch      392 Loss: 0.05485957954078913%\n",
      "Epoch      393 Loss: 0.05132629885338247%\n",
      "Epoch      394 Loss: 0.04801687609869987%\n",
      "Epoch      395 Loss: 0.04491608415264636%\n",
      "Epoch      396 Loss: 0.042013044003397226%\n",
      "Epoch      397 Loss: 0.03929389058612287%\n",
      "Epoch      398 Loss: 0.03674815234262496%\n",
      "Epoch      399 Loss: 0.03436485421843827%\n",
      "Epoch      400 Loss: 0.032133745844475925%\n",
      "Epoch      401 Loss: 0.030046599567867815%\n",
      "Epoch      402 Loss: 0.028092128923162818%\n",
      "Epoch      403 Loss: 0.026263625477440655%\n",
      "Epoch      404 Loss: 0.02455209323670715%\n",
      "Epoch      405 Loss: 0.022951289429329336%\n",
      "Epoch      406 Loss: 0.021453655790537596%\n",
      "Epoch      407 Loss: 0.020052268519066274%\n",
      "Epoch      408 Loss: 0.018741840904112905%\n",
      "Epoch      409 Loss: 0.0175155044416897%\n",
      "Epoch      410 Loss: 0.01636922243051231%\n",
      "Epoch      411 Loss: 0.01529724249849096%\n",
      "Epoch      412 Loss: 0.01429460826329887%\n",
      "Epoch      413 Loss: 0.013356523413676769%\n",
      "Epoch      414 Loss: 0.0124803525977768%\n",
      "Epoch      415 Loss: 0.011660661402856931%\n",
      "Epoch      416 Loss: 0.010894054867094383%\n",
      "Epoch      417 Loss: 0.010177850344916806%\n",
      "Epoch      418 Loss: 0.00950853354879655%\n",
      "Epoch      419 Loss: 0.00888263966771774%\n",
      "Epoch      420 Loss: 0.008298022294184193%\n",
      "Epoch      421 Loss: 0.007751397788524628%\n",
      "Epoch      422 Loss: 0.0072402748628519475%\n",
      "Epoch      423 Loss: 0.006762944394722581%\n",
      "Epoch      424 Loss: 0.006317040970316157%\n",
      "Epoch      425 Loss: 0.005899886309634894%\n",
      "Epoch      426 Loss: 0.005510245682671666%\n",
      "Epoch      427 Loss: 0.005146435432834551%\n",
      "Epoch      428 Loss: 0.004806364813703112%\n",
      "Epoch      429 Loss: 0.0044890122808283195%\n",
      "Epoch      430 Loss: 0.004191985499346629%\n",
      "Epoch      431 Loss: 0.003914703484042548%\n",
      "Epoch      432 Loss: 0.0036557685234583914%\n",
      "Epoch      433 Loss: 0.003413767262827605%\n",
      "Epoch      434 Loss: 0.003187754555256106%\n",
      "Epoch      435 Loss: 0.002976371433760505%\n",
      "Epoch      436 Loss: 0.002779305214062333%\n",
      "Epoch      437 Loss: 0.0025950270355679095%\n",
      "Epoch      438 Loss: 0.002422845682303887%\n",
      "Epoch      439 Loss: 0.0022622689357376657%\n",
      "Epoch      440 Loss: 0.0021122346879565157%\n",
      "Epoch      441 Loss: 0.0019722683646250516%\n",
      "Epoch      442 Loss: 0.0018412407371215522%\n",
      "Epoch      443 Loss: 0.0017190384824061766%\n",
      "Epoch      444 Loss: 0.0016048499674070626%\n",
      "Epoch      445 Loss: 0.0014982517313910648%\n",
      "Epoch      446 Loss: 0.0013986134035803843%\n",
      "Epoch      447 Loss: 0.001305782552663004%\n",
      "Epoch      448 Loss: 0.001219071782543324%\n",
      "Epoch      449 Loss: 0.0011380125215509906%\n",
      "Epoch      450 Loss: 0.0010623821253830101%\n",
      "Epoch      451 Loss: 0.0009916406270349398%\n",
      "Epoch      452 Loss: 0.0009258010322810151%\n",
      "Epoch      453 Loss: 0.000864289359014947%\n",
      "Epoch      454 Loss: 0.0008067871021921746%\n",
      "Epoch      455 Loss: 0.0007530621587648056%\n",
      "Epoch      456 Loss: 0.0007029254902590765%\n",
      "Epoch      457 Loss: 0.0006561789632542059%\n",
      "Epoch      458 Loss: 0.0006125297659309581%\n",
      "Epoch      459 Loss: 0.0005717358817491913%\n",
      "Epoch      460 Loss: 0.0005336686626833398%\n",
      "Epoch      461 Loss: 0.0004981625352229457%\n",
      "Epoch      462 Loss: 0.0004649961283575976%\n",
      "Epoch      463 Loss: 0.00043398968045949005%\n",
      "Epoch      464 Loss: 0.0004051320502185263%\n",
      "Epoch      465 Loss: 0.00037813592825841624%\n",
      "Epoch      466 Loss: 0.00035294340250402456%\n",
      "Epoch      467 Loss: 0.000329386875819182%\n",
      "Epoch      468 Loss: 0.00030744804462301545%\n",
      "Epoch      469 Loss: 0.00028692111300188117%\n",
      "Epoch      470 Loss: 0.0002678246573850629%\n",
      "Epoch      471 Loss: 0.00025000740606628824%\n",
      "Epoch      472 Loss: 0.00023332202090387%\n",
      "Epoch      473 Loss: 0.00021772264062747126%\n",
      "Epoch      474 Loss: 0.00020319787381595233%\n",
      "Epoch      475 Loss: 0.00018963380625791615%\n",
      "Epoch      476 Loss: 0.0001770137373569014%\n",
      "Epoch      477 Loss: 0.00016521100860700244%\n",
      "Epoch      478 Loss: 0.00015416310361615615%\n",
      "Epoch      479 Loss: 0.00014391905551747186%\n",
      "Epoch      480 Loss: 0.00013428950751404045%\n",
      "Epoch      481 Loss: 0.00012536681879282696%\n",
      "Epoch      482 Loss: 0.00011700213917720248%\n",
      "Epoch      483 Loss: 0.00010919389978880645%\n",
      "Epoch      484 Loss: 0.00010191384944846504%\n",
      "Epoch      485 Loss: 9.512892802376882e-05%\n",
      "Epoch      486 Loss: 8.876293122739298e-05%\n",
      "Epoch      487 Loss: 8.283423085231334e-05%\n",
      "Epoch      488 Loss: 7.731468940619379e-05%\n",
      "Epoch      489 Loss: 7.21726735264383e-05%\n",
      "Epoch      490 Loss: 6.735914439559565e-05%\n",
      "Epoch      491 Loss: 6.285812901296595e-05%\n",
      "Epoch      492 Loss: 5.866785954822262e-05%\n",
      "Epoch      493 Loss: 5.474994395626709e-05%\n",
      "Epoch      494 Loss: 5.110792926643626e-05%\n",
      "Epoch      495 Loss: 4.767352947965264e-05%\n",
      "Epoch      496 Loss: 4.4505574692266237e-05%\n",
      "Epoch      497 Loss: 4.1530989847160527e-05%\n",
      "Epoch      498 Loss: 3.8755695186409866e-05%\n",
      "Epoch      499 Loss: 3.6158749594505935e-05%\n",
      "Epoch      500 Loss: 3.376952122380317e-05%\n",
      "Epoch      501 Loss: 3.150207987800968e-05%\n",
      "Epoch      502 Loss: 2.9394163902907167e-05%\n",
      "Epoch      503 Loss: 2.744660037024005e-05%\n",
      "Epoch      504 Loss: 2.559894767273363e-05%\n",
      "Epoch      505 Loss: 2.390592896972521e-05%\n",
      "Epoch      506 Loss: 2.2298965518530167e-05%\n",
      "Epoch      507 Loss: 2.0813116918816377e-05%\n",
      "Epoch      508 Loss: 1.941524487847346e-05%\n",
      "Epoch      509 Loss: 1.8119203559763264e-05%\n",
      "Epoch      510 Loss: 1.6914469824769185e-05%\n",
      "Epoch      511 Loss: 1.578194428475399e-05%\n",
      "Epoch      512 Loss: 1.473895849812834e-05%\n",
      "Epoch      513 Loss: 1.3747090576998744e-05%\n",
      "Epoch      514 Loss: 1.2832437334964197e-05%\n",
      "Epoch      515 Loss: 1.1967819091296406e-05%\n",
      "Epoch      516 Loss: 1.1175157510479039e-05%\n",
      "Epoch      517 Loss: 1.0423119078950549e-05%\n",
      "Epoch      518 Loss: 9.73257243686021e-06%\n",
      "Epoch      519 Loss: 9.08903672325323e-06%\n",
      "Epoch      520 Loss: 8.481391944314964e-06%\n",
      "Epoch      521 Loss: 7.914877642178908e-06%\n",
      "Epoch      522 Loss: 7.382511313380746e-06%\n",
      "Epoch      523 Loss: 6.89682480015108e-06%\n",
      "Epoch      524 Loss: 6.4367327468062285e-06%\n",
      "Epoch      525 Loss: 6.002739638688581e-06%\n",
      "Epoch      526 Loss: 5.5993965020206815e-06%\n",
      "Epoch      527 Loss: 5.222331367349398e-06%\n",
      "Epoch      528 Loss: 4.875502668255649e-06%\n",
      "Epoch      529 Loss: 4.5520280878008634e-06%\n",
      "Epoch      530 Loss: 4.254395946645673e-06%\n",
      "Epoch      531 Loss: 3.964451877891406e-06%\n",
      "Epoch      532 Loss: 3.7065106539557746e-06%\n",
      "Epoch      533 Loss: 3.4583536034915596e-06%\n",
      "Epoch      534 Loss: 3.2209332090360476e-06%\n",
      "Epoch      535 Loss: 3.0074396306645212e-06%\n",
      "Epoch      536 Loss: 2.8032619781015455e-06%\n",
      "Epoch      537 Loss: 2.621679584535741e-06%\n",
      "Epoch      538 Loss: 2.4452447178191505e-06%\n",
      "Epoch      539 Loss: 2.283053746054975e-06%\n",
      "Epoch      540 Loss: 2.127297094034475e-06%\n",
      "Epoch      541 Loss: 1.98459275679852e-06%\n",
      "Epoch      542 Loss: 1.8533285128796706e-06%\n",
      "Epoch      543 Loss: 1.7289053744207195e-06%\n",
      "Epoch      544 Loss: 1.6103175681791981e-06%\n",
      "Epoch      545 Loss: 1.503241620071094e-06%\n",
      "Epoch      546 Loss: 1.4040836049389327e-06%\n",
      "Epoch      547 Loss: 1.312402275743807e-06%\n",
      "Epoch      548 Loss: 1.2251351932945909e-06%\n",
      "Epoch      549 Loss: 1.143418160154397e-06%\n",
      "Epoch      550 Loss: 1.0669825911691078e-06%\n",
      "Epoch      551 Loss: 9.961632407851084e-07%\n",
      "Epoch      552 Loss: 9.289241376109203e-07%\n",
      "Epoch      553 Loss: 8.662518702351463e-07%\n",
      "Epoch      554 Loss: 8.079098279267782e-07%\n",
      "Epoch      555 Loss: 7.552230840701668e-07%\n",
      "Epoch      556 Loss: 7.02812830155608e-07%\n",
      "Epoch      557 Loss: 6.575931799090995e-07%\n",
      "Epoch      558 Loss: 6.120104423246175e-07%\n",
      "Epoch      559 Loss: 5.725660834343671e-07%\n",
      "Epoch      560 Loss: 5.339998665476742e-07%\n",
      "Epoch      561 Loss: 4.976187462091275e-07%\n",
      "Epoch      562 Loss: 4.6333212821991765e-07%\n",
      "Epoch      563 Loss: 4.3222634360517986e-07%\n",
      "Epoch      564 Loss: 4.0333612005838404e-07%\n",
      "Epoch      565 Loss: 3.7690739418394514e-07%\n",
      "Epoch      566 Loss: 3.510209012347332e-07%\n",
      "Epoch      567 Loss: 3.2844207353832644e-07%\n",
      "Epoch      568 Loss: 3.079353660950801e-07%\n",
      "Epoch      569 Loss: 2.8681137465014217e-07%\n",
      "Epoch      570 Loss: 2.6859590107619624e-07%\n",
      "Epoch      571 Loss: 2.5008297654949274e-07%\n",
      "Epoch      572 Loss: 2.3395765325062712e-07%\n",
      "Epoch      573 Loss: 2.1892709867188387e-07%\n",
      "Epoch      574 Loss: 2.0466508487970714e-07%\n",
      "Epoch      575 Loss: 1.9062307288209013e-07%\n",
      "Epoch      576 Loss: 1.7733094992422593e-07%\n",
      "Epoch      577 Loss: 1.6718928463888005e-07%\n",
      "Epoch      578 Loss: 1.5499077576919262e-07%\n",
      "Epoch      579 Loss: 1.4483783061791655e-07%\n",
      "Epoch      580 Loss: 1.3437251311643195e-07%\n",
      "Epoch      581 Loss: 1.2683409877922713e-07%\n",
      "Epoch      582 Loss: 1.193073195793204e-07%\n",
      "Epoch      583 Loss: 1.1061871418860392e-07%\n",
      "Epoch      584 Loss: 1.0340537315300935e-07%\n",
      "Epoch      585 Loss: 9.717702198486222e-08%\n",
      "Epoch      586 Loss: 9.09622599465365e-08%\n",
      "Epoch      587 Loss: 8.443246102274315e-08%\n",
      "Epoch      588 Loss: 7.847944516470307e-08%\n",
      "Epoch      589 Loss: 7.29048821312972e-08%\n",
      "Epoch      590 Loss: 6.87805368215777e-08%\n",
      "Epoch      591 Loss: 6.371871918986471e-08%\n",
      "Epoch      592 Loss: 5.928493251872169e-08%\n",
      "Epoch      593 Loss: 5.5150906064227456e-08%\n",
      "Epoch      594 Loss: 5.170699424184022e-08%\n",
      "Epoch      595 Loss: 4.759073135573999e-08%\n",
      "Epoch      596 Loss: 4.427000988016516e-08%\n",
      "Epoch      597 Loss: 4.119025120985498e-08%\n",
      "Epoch      598 Loss: 3.857190122857901e-08%\n",
      "Epoch      599 Loss: 3.6266190051037483e-08%\n",
      "Epoch      600 Loss: 3.4141578453272814e-08%\n",
      "Epoch      601 Loss: 3.133813208933134e-08%\n",
      "Epoch      602 Loss: 2.987832203871221e-08%\n",
      "Epoch      603 Loss: 2.765467854715098e-08%\n",
      "Epoch      604 Loss: 2.5899282718455652e-08%\n",
      "Epoch      605 Loss: 2.4201440851356892e-08%\n",
      "Epoch      606 Loss: 2.2382451447811036e-08%\n",
      "Epoch      607 Loss: 2.10648387621859e-08%\n",
      "Epoch      608 Loss: 1.978719410544727e-08%\n",
      "Epoch      609 Loss: 1.871223176408421e-08%\n",
      "Epoch      610 Loss: 1.7351808878629527e-08%\n",
      "Epoch      611 Loss: 1.6270007563434774e-08%\n",
      "Epoch      612 Loss: 1.5223022842292266e-08%\n",
      "Epoch      613 Loss: 1.4139889259467964e-08%\n",
      "Epoch      614 Loss: 1.3233503182163986e-08%\n",
      "Epoch      615 Loss: 1.2490009027033011e-08%\n",
      "Epoch      616 Loss: 1.1897682838934998e-08%\n",
      "Epoch      617 Loss: 1.0756195933936397e-08%\n",
      "Epoch      618 Loss: 1.0510348147363402e-08%\n",
      "Epoch      619 Loss: 9.497203024011469e-09%\n",
      "Epoch      620 Loss: 8.701395159960157e-09%\n",
      "Epoch      621 Loss: 8.208189683500677e-09%\n",
      "Epoch      622 Loss: 7.366907084360719e-09%\n",
      "Epoch      623 Loss: 7.063150064823276e-09%\n",
      "Epoch      624 Loss: 6.52287113211969e-09%\n",
      "Epoch      625 Loss: 6.143441311223796e-09%\n",
      "Epoch      626 Loss: 5.730171892537328e-09%\n",
      "Epoch      627 Loss: 5.595612861952759e-09%\n",
      "Epoch      628 Loss: 5.374900524657278e-09%\n",
      "Epoch      629 Loss: 4.9467985263618175e-09%\n",
      "Epoch      630 Loss: 4.53646009646036e-09%\n",
      "Epoch      631 Loss: 4.029576672337498e-09%\n",
      "Epoch      632 Loss: 3.660094449742246e-09%\n",
      "Epoch      633 Loss: 3.624123223744391e-09%\n",
      "Epoch      634 Loss: 3.2741809263825417e-09%\n",
      "Epoch      635 Loss: 3.206324095117452e-09%\n",
      "Epoch      636 Loss: 3.0397906414236786e-09%\n",
      "Epoch      637 Loss: 2.8458124745611713e-09%\n",
      "Epoch      638 Loss: 2.751221472863108e-09%\n",
      "Epoch      639 Loss: 2.5367263845055277e-09%\n",
      "Epoch      640 Loss: 2.3022472817046946e-09%\n",
      "Epoch      641 Loss: 2.1614710021822248e-09%\n",
      "Epoch      642 Loss: 1.971844909576248e-09%\n",
      "Epoch      643 Loss: 1.7909229654833325e-09%\n",
      "Epoch      644 Loss: 1.4780177082229784e-09%\n",
      "Epoch      645 Loss: 1.3877787807814457e-09%\n",
      "Epoch      646 Loss: 1.1951328815484885e-09%\n",
      "Epoch      647 Loss: 1.0552447804457188e-09%\n",
      "Epoch      648 Loss: 9.979572723750607e-10%\n",
      "Epoch      649 Loss: 9.422684854598629e-10%\n",
      "Epoch      650 Loss: 8.530065542800003e-10%\n",
      "Epoch      651 Loss: 7.51754214434186e-10%\n",
      "Epoch      652 Loss: 7.355005493536737e-10%\n",
      "Epoch      653 Loss: 6.568967592102126e-10%\n",
      "Epoch      654 Loss: 6.417089082333405e-10%\n",
      "Epoch      655 Loss: 5.266009850402043e-10%\n",
      "Epoch      656 Loss: 5.266009850402043e-10%\n",
      "Epoch      657 Loss: 4.733102798581967e-10%\n",
      "Epoch      658 Loss: 4.733102798581967e-10%\n",
      "Epoch      659 Loss: 4.4773074137083313e-10%\n",
      "Epoch      660 Loss: 4.106937012693379e-10%\n",
      "Epoch      661 Loss: 3.752553823233029e-10%\n",
      "Epoch      662 Loss: 3.525180147789797e-10%\n",
      "Epoch      663 Loss: 2.9878322038712213e-10%\n",
      "Epoch      664 Loss: 3.304911899704166e-10%\n",
      "Epoch      665 Loss: 2.885691685605707e-10%\n",
      "Epoch      666 Loss: 2.7853275241795927e-10%\n",
      "Epoch      667 Loss: 2.3101520696400257e-10%\n",
      "Epoch      668 Loss: 2.1325163857000007e-10%\n",
      "Epoch      669 Loss: 2.3101520696400257e-10%\n",
      "Epoch      670 Loss: 2.0463630789890885e-10%\n",
      "Epoch      671 Loss: 1.7985612998927536e-10%\n",
      "Epoch      672 Loss: 1.7985612998927536e-10%\n",
      "Epoch      673 Loss: 1.7195134205394424e-10%\n",
      "Epoch      674 Loss: 1.566746732351021e-10%\n",
      "Epoch      675 Loss: 1.2825296380469808e-10%\n",
      "Epoch      676 Loss: 1.0267342531733448e-10%\n",
      "Epoch      677 Loss: 1.0880185641326534e-10%\n",
      "Epoch      678 Loss: 1.2825296380469808e-10%\n",
      "Epoch      679 Loss: 1.0267342531733448e-10%\n",
      "Epoch      680 Loss: 1.0880185641326534e-10%\n",
      "Epoch      681 Loss: 1.0880185641326534e-10%\n",
      "Epoch      682 Loss: 1.0267342531733448e-10%\n",
      "Epoch      683 Loss: 1.0880185641326534e-10%\n",
      "Epoch      684 Loss: 1.0880185641326534e-10%\n",
      "Epoch      685 Loss: 9.094947017729282e-11%\n",
      "Epoch      686 Loss: 1.0267342531733448e-10%\n",
      "Epoch      687 Loss: 7.993605777301127e-11%\n",
      "Epoch      688 Loss: 7.469580509678053e-11%\n",
      "Epoch      689 Loss: 9.094947017729282e-11%\n",
      "Epoch      690 Loss: 9.672262990534364e-11%\n",
      "Epoch      691 Loss: 7.993605777301127e-11%\n",
      "Epoch      692 Loss: 6.474820679613913e-11%\n",
      "Epoch      693 Loss: 7.469580509678053e-11%\n",
      "Epoch      694 Loss: 7.993605777301127e-11%\n",
      "Epoch      695 Loss: 8.535394613318203e-11%\n",
      "Epoch      696 Loss: 1.0267342531733448e-10%\n",
      "Epoch      697 Loss: 8.535394613318203e-11%\n",
      "Epoch      698 Loss: 9.094947017729282e-11%\n",
      "Epoch      699 Loss: 8.535394613318203e-11%\n",
      "Epoch      700 Loss: 9.094947017729282e-11%\n",
      "Epoch      701 Loss: 8.535394613318203e-11%\n",
      "Epoch      702 Loss: 7.469580509678053e-11%\n",
      "Epoch      703 Loss: 7.993605777301127e-11%\n",
      "Epoch      704 Loss: 6.963318810448982e-11%\n",
      "Epoch      705 Loss: 7.469580509678053e-11%\n",
      "Epoch      706 Loss: 6.004086117172847e-11%\n",
      "Epoch      707 Loss: 5.551115123125783e-11%\n",
      "Epoch      708 Loss: 5.551115123125783e-11%\n",
      "Epoch      709 Loss: 4.6984638402136625e-11%\n",
      "Epoch      710 Loss: 5.551115123125783e-11%\n",
      "Epoch      711 Loss: 5.551115123125783e-11%\n",
      "Epoch      712 Loss: 6.963318810448982e-11%\n",
      "Epoch      713 Loss: 4.6984638402136625e-11%\n",
      "Epoch      714 Loss: 6.004086117172847e-11%\n",
      "Epoch      715 Loss: 5.551115123125783e-11%\n",
      "Epoch      716 Loss: 5.1159076974727213e-11%\n",
      "Epoch      717 Loss: 4.298783551348606e-11%\n",
      "Epoch      718 Loss: 3.552713678800501e-11%\n",
      "Epoch      719 Loss: 5.1159076974727213e-11%\n",
      "Epoch      720 Loss: 3.206324095117452e-11%\n",
      "Epoch      721 Loss: 3.206324095117452e-11%\n",
      "Epoch      722 Loss: 3.916866830877552e-11%\n",
      "Epoch      723 Loss: 3.916866830877552e-11%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch      724 Loss: 3.552713678800501e-11%\n",
      "Epoch      725 Loss: 3.552713678800501e-11%\n",
      "Epoch      726 Loss: 3.552713678800501e-11%\n",
      "Epoch      727 Loss: 3.552713678800501e-11%\n",
      "Epoch      728 Loss: 3.552713678800501e-11%\n",
      "Epoch      729 Loss: 3.552713678800501e-11%\n",
      "Epoch      730 Loss: 3.552713678800501e-11%\n",
      "Epoch      731 Loss: 3.552713678800501e-11%\n",
      "Epoch      732 Loss: 3.206324095117452e-11%\n",
      "Epoch      733 Loss: 3.552713678800501e-11%\n",
      "Epoch      734 Loss: 2.8776980798284058e-11%\n",
      "Epoch      735 Loss: 2.566835632933362e-11%\n",
      "Epoch      736 Loss: 2.566835632933362e-11%\n",
      "Epoch      737 Loss: 3.206324095117452e-11%\n",
      "Epoch      738 Loss: 3.206324095117452e-11%\n",
      "Epoch      739 Loss: 3.206324095117452e-11%\n",
      "Epoch      740 Loss: 3.206324095117452e-11%\n",
      "Epoch      741 Loss: 2.8776980798284058e-11%\n",
      "Epoch      742 Loss: 2.8776980798284058e-11%\n",
      "Epoch      743 Loss: 2.2737367544323206e-11%\n",
      "Epoch      744 Loss: 2.566835632933362e-11%\n",
      "Epoch      745 Loss: 2.566835632933362e-11%\n",
      "Epoch      746 Loss: 1.9984014443252818e-11%\n",
      "Epoch      747 Loss: 1.9984014443252818e-11%\n",
      "Epoch      748 Loss: 2.566835632933362e-11%\n",
      "Epoch      749 Loss: 2.566835632933362e-11%\n",
      "Epoch      750 Loss: 2.2737367544323206e-11%\n",
      "Epoch      751 Loss: 2.2737367544323206e-11%\n",
      "Epoch      752 Loss: 1.9984014443252818e-11%\n",
      "Epoch      753 Loss: 1.7408297026122455e-11%\n",
      "Epoch      754 Loss: 2.2737367544323206e-11%\n",
      "Epoch      755 Loss: 2.566835632933362e-11%\n",
      "Epoch      756 Loss: 3.206324095117452e-11%\n",
      "Epoch      757 Loss: 1.9984014443252818e-11%\n",
      "Epoch      758 Loss: 2.2737367544323206e-11%\n",
      "Epoch      759 Loss: 2.2737367544323206e-11%\n",
      "Epoch      760 Loss: 2.566835632933362e-11%\n",
      "Epoch      761 Loss: 2.566835632933362e-11%\n",
      "Epoch      762 Loss: 1.9984014443252818e-11%\n",
      "Epoch      763 Loss: 1.9984014443252818e-11%\n",
      "Epoch      764 Loss: 1.9984014443252818e-11%\n",
      "Epoch      765 Loss: 1.9984014443252818e-11%\n",
      "Epoch      766 Loss: 1.9984014443252818e-11%\n",
      "Epoch      767 Loss: 1.9984014443252818e-11%\n",
      "Epoch      768 Loss: 2.566835632933362e-11%\n",
      "Epoch      769 Loss: 2.566835632933362e-11%\n",
      "Epoch      770 Loss: 2.566835632933362e-11%\n",
      "Epoch      771 Loss: 2.8776980798284058e-11%\n",
      "Epoch      772 Loss: 2.8776980798284058e-11%\n",
      "Epoch      773 Loss: 2.8776980798284058e-11%\n",
      "Epoch      774 Loss: 2.8776980798284058e-11%\n",
      "Epoch      775 Loss: 3.552713678800501e-11%\n",
      "Epoch      776 Loss: 3.552713678800501e-11%\n",
      "Epoch      777 Loss: 3.552713678800501e-11%\n",
      "Epoch      778 Loss: 3.206324095117452e-11%\n",
      "Epoch      779 Loss: 3.206324095117452e-11%\n",
      "Epoch      780 Loss: 4.6984638402136625e-11%\n",
      "Epoch      781 Loss: 3.206324095117452e-11%\n",
      "Epoch      782 Loss: 3.206324095117452e-11%\n",
      "Epoch      783 Loss: 3.206324095117452e-11%\n",
      "Epoch      784 Loss: 3.206324095117452e-11%\n",
      "Epoch      785 Loss: 3.206324095117452e-11%\n",
      "Epoch      786 Loss: 3.206324095117452e-11%\n",
      "Epoch      787 Loss: 2.566835632933362e-11%\n",
      "Epoch      788 Loss: 3.206324095117452e-11%\n",
      "Epoch      789 Loss: 1.9984014443252818e-11%\n",
      "Epoch      790 Loss: 1.9984014443252818e-11%\n",
      "Epoch      791 Loss: 2.8776980798284058e-11%\n",
      "Epoch      792 Loss: 2.8776980798284058e-11%\n",
      "Epoch      793 Loss: 3.206324095117452e-11%\n",
      "Epoch      794 Loss: 3.206324095117452e-11%\n",
      "Epoch      795 Loss: 2.8776980798284058e-11%\n",
      "Epoch      796 Loss: 2.8776980798284058e-11%\n",
      "Epoch      797 Loss: 2.8776980798284058e-11%\n",
      "Epoch      798 Loss: 2.8776980798284058e-11%\n",
      "Epoch      799 Loss: 2.8776980798284058e-11%\n",
      "Epoch      800 Loss: 2.566835632933362e-11%\n",
      "Epoch      801 Loss: 2.8776980798284058e-11%\n",
      "Epoch      802 Loss: 2.8776980798284058e-11%\n",
      "Epoch      803 Loss: 2.8776980798284058e-11%\n",
      "Epoch      804 Loss: 2.8776980798284058e-11%\n",
      "Epoch      805 Loss: 2.8776980798284058e-11%\n",
      "Epoch      806 Loss: 2.8776980798284058e-11%\n",
      "Epoch      807 Loss: 2.566835632933362e-11%\n",
      "Epoch      808 Loss: 2.8776980798284058e-11%\n",
      "Epoch      809 Loss: 2.8776980798284058e-11%\n",
      "Epoch      810 Loss: 2.8776980798284058e-11%\n",
      "Epoch      811 Loss: 2.2737367544323206e-11%\n",
      "Epoch      812 Loss: 2.2737367544323206e-11%\n",
      "Epoch      813 Loss: 2.2737367544323206e-11%\n",
      "Epoch      814 Loss: 2.2737367544323206e-11%\n",
      "Epoch      815 Loss: 1.7408297026122455e-11%\n",
      "Epoch      816 Loss: 1.7408297026122455e-11%\n",
      "Epoch      817 Loss: 1.7408297026122455e-11%\n",
      "Epoch      818 Loss: 1.7408297026122455e-11%\n",
      "Epoch      819 Loss: 1.9984014443252818e-11%\n",
      "Epoch      820 Loss: 2.566835632933362e-11%\n",
      "Epoch      821 Loss: 2.566835632933362e-11%\n",
      "Epoch      822 Loss: 2.566835632933362e-11%\n",
      "Epoch      823 Loss: 2.2737367544323206e-11%\n",
      "Epoch      824 Loss: 2.2737367544323206e-11%\n",
      "Epoch      825 Loss: 2.2737367544323206e-11%\n",
      "Epoch      826 Loss: 2.2737367544323206e-11%\n",
      "Epoch      827 Loss: 2.2737367544323206e-11%\n",
      "Epoch      828 Loss: 2.2737367544323206e-11%\n",
      "Epoch      829 Loss: 1.7408297026122455e-11%\n",
      "Epoch      830 Loss: 2.2737367544323206e-11%\n",
      "Epoch      831 Loss: 2.566835632933362e-11%\n",
      "Epoch      832 Loss: 1.9984014443252818e-11%\n",
      "Epoch      833 Loss: 1.7408297026122455e-11%\n",
      "Epoch      834 Loss: 1.7408297026122455e-11%\n",
      "Epoch      835 Loss: 1.5010215292932116e-11%\n",
      "Epoch      836 Loss: 1.5010215292932116e-11%\n",
      "Epoch      837 Loss: 1.2789769243681803e-11%\n",
      "Epoch      838 Loss: 1.7408297026122455e-11%\n",
      "Epoch      839 Loss: 1.2789769243681803e-11%\n",
      "Epoch      840 Loss: 1.2789769243681803e-11%\n",
      "Epoch      841 Loss: 1.2789769243681803e-11%\n",
      "Epoch      842 Loss: 1.2789769243681803e-11%\n",
      "Epoch      843 Loss: 1.5010215292932116e-11%\n",
      "Epoch      844 Loss: 1.2789769243681803e-11%\n",
      "Epoch      845 Loss: 1.2789769243681803e-11%\n",
      "Epoch      846 Loss: 1.2789769243681803e-11%\n",
      "Epoch      847 Loss: 1.0746958878371515e-11%\n",
      "Epoch      848 Loss: 8.881784197001252e-12%\n",
      "Epoch      849 Loss: 8.881784197001252e-12%\n",
      "Epoch      850 Loss: 7.194245199571014e-12%\n",
      "Epoch      851 Loss: 7.194245199571014e-12%\n",
      "Epoch      852 Loss: 7.194245199571014e-12%\n",
      "Epoch      853 Loss: 7.194245199571014e-12%\n",
      "Epoch      854 Loss: 7.194245199571014e-12%\n",
      "Epoch      855 Loss: 1.0746958878371515e-11%\n",
      "Epoch      856 Loss: 1.0746958878371515e-11%\n",
      "Epoch      857 Loss: 1.2789769243681803e-11%\n",
      "Epoch      858 Loss: 1.0746958878371515e-11%\n",
      "Epoch      859 Loss: 1.2789769243681803e-11%\n",
      "Epoch      860 Loss: 1.2789769243681803e-11%\n",
      "Epoch      861 Loss: 1.2789769243681803e-11%\n",
      "Epoch      862 Loss: 1.2789769243681803e-11%\n",
      "Epoch      863 Loss: 1.2789769243681803e-11%\n",
      "Epoch      864 Loss: 1.2789769243681803e-11%\n",
      "Epoch      865 Loss: 1.2789769243681803e-11%\n",
      "Epoch      866 Loss: 1.2789769243681803e-11%\n",
      "Epoch      867 Loss: 1.2789769243681803e-11%\n",
      "Epoch      868 Loss: 1.2789769243681803e-11%\n",
      "Epoch      869 Loss: 1.2789769243681803e-11%\n",
      "Epoch      870 Loss: 1.2789769243681803e-11%\n",
      "Epoch      871 Loss: 1.2789769243681803e-11%\n",
      "Epoch      872 Loss: 1.5010215292932116e-11%\n",
      "Epoch      873 Loss: 1.5010215292932116e-11%\n",
      "Epoch      874 Loss: 1.2789769243681803e-11%\n",
      "Epoch      875 Loss: 1.2789769243681803e-11%\n",
      "Epoch      876 Loss: 1.5010215292932116e-11%\n",
      "Epoch      877 Loss: 1.2789769243681803e-11%\n",
      "Epoch      878 Loss: 1.0746958878371515e-11%\n",
      "Epoch      879 Loss: 1.0746958878371515e-11%\n",
      "Epoch      880 Loss: 1.0746958878371515e-11%\n",
      "Epoch      881 Loss: 1.2789769243681803e-11%\n",
      "Epoch      882 Loss: 1.0746958878371515e-11%\n",
      "Epoch      883 Loss: 1.0746958878371515e-11%\n",
      "Epoch      884 Loss: 1.0746958878371515e-11%\n",
      "Epoch      885 Loss: 1.0746958878371515e-11%\n",
      "Epoch      886 Loss: 1.0746958878371515e-11%\n",
      "Epoch      887 Loss: 1.0746958878371515e-11%\n",
      "Epoch      888 Loss: 1.0746958878371515e-11%\n",
      "Epoch      889 Loss: 1.0746958878371515e-11%\n",
      "Epoch      890 Loss: 8.881784197001252e-12%\n",
      "Epoch      891 Loss: 8.881784197001252e-12%\n",
      "Epoch      892 Loss: 8.881784197001252e-12%\n",
      "Epoch      893 Loss: 1.0746958878371515e-11%\n",
      "Epoch      894 Loss: 1.0746958878371515e-11%\n",
      "Epoch      895 Loss: 1.0746958878371515e-11%\n",
      "Epoch      896 Loss: 1.0746958878371515e-11%\n",
      "Epoch      897 Loss: 1.0746958878371515e-11%\n",
      "Epoch      898 Loss: 8.881784197001252e-12%\n",
      "Epoch      899 Loss: 7.194245199571014e-12%\n",
      "Epoch      900 Loss: 7.194245199571014e-12%\n",
      "Epoch      901 Loss: 7.194245199571014e-12%\n",
      "Epoch      902 Loss: 7.194245199571014e-12%\n",
      "Epoch      903 Loss: 7.194245199571014e-12%\n",
      "Epoch      904 Loss: 7.194245199571014e-12%\n",
      "Epoch      905 Loss: 7.194245199571014e-12%\n",
      "Epoch      906 Loss: 7.194245199571014e-12%\n",
      "Epoch      907 Loss: 7.194245199571014e-12%\n",
      "Epoch      908 Loss: 7.194245199571014e-12%\n",
      "Epoch      909 Loss: 7.194245199571014e-12%\n",
      "Epoch      910 Loss: 7.194245199571014e-12%\n",
      "Epoch      911 Loss: 7.194245199571014e-12%\n",
      "Epoch      912 Loss: 7.194245199571014e-12%\n",
      "Epoch      913 Loss: 7.194245199571014e-12%\n",
      "Epoch      914 Loss: 7.194245199571014e-12%\n",
      "Epoch      915 Loss: 7.194245199571014e-12%\n",
      "Epoch      916 Loss: 7.194245199571014e-12%\n",
      "Epoch      917 Loss: 7.194245199571014e-12%\n",
      "Epoch      918 Loss: 7.194245199571014e-12%\n",
      "Epoch      919 Loss: 7.194245199571014e-12%\n",
      "Epoch      920 Loss: 8.881784197001252e-12%\n",
      "Epoch      921 Loss: 8.881784197001252e-12%\n",
      "Epoch      922 Loss: 8.881784197001252e-12%\n",
      "Epoch      923 Loss: 8.881784197001252e-12%\n",
      "Epoch      924 Loss: 8.881784197001252e-12%\n",
      "Epoch      925 Loss: 8.881784197001252e-12%\n",
      "Epoch      926 Loss: 8.881784197001252e-12%\n",
      "Epoch      927 Loss: 8.881784197001252e-12%\n",
      "Epoch      928 Loss: 8.881784197001252e-12%\n",
      "Epoch      929 Loss: 8.881784197001252e-12%\n",
      "Epoch      930 Loss: 8.881784197001252e-12%\n",
      "Epoch      931 Loss: 8.881784197001252e-12%\n",
      "Epoch      932 Loss: 8.881784197001252e-12%\n",
      "Epoch      933 Loss: 8.881784197001252e-12%\n",
      "Epoch      934 Loss: 8.881784197001252e-12%\n",
      "Epoch      935 Loss: 8.881784197001252e-12%\n",
      "Epoch      936 Loss: 8.881784197001252e-12%\n",
      "Epoch      937 Loss: 8.881784197001252e-12%\n",
      "Epoch      938 Loss: 8.881784197001252e-12%\n",
      "Epoch      939 Loss: 8.881784197001252e-12%\n",
      "Epoch      940 Loss: 8.881784197001252e-12%\n",
      "Epoch      941 Loss: 8.881784197001252e-12%\n",
      "Epoch      942 Loss: 8.881784197001252e-12%\n",
      "Epoch      943 Loss: 8.881784197001252e-12%\n",
      "Epoch      944 Loss: 8.881784197001252e-12%\n",
      "Epoch      945 Loss: 8.881784197001252e-12%\n",
      "Epoch      946 Loss: 8.881784197001252e-12%\n",
      "Epoch      947 Loss: 8.881784197001252e-12%\n",
      "Epoch      948 Loss: 8.881784197001252e-12%\n",
      "Epoch      949 Loss: 8.881784197001252e-12%\n",
      "Epoch      950 Loss: 8.881784197001252e-12%\n",
      "Epoch      951 Loss: 8.881784197001252e-12%\n",
      "Epoch      952 Loss: 8.881784197001252e-12%\n",
      "Epoch      953 Loss: 8.881784197001252e-12%\n",
      "Epoch      954 Loss: 8.881784197001252e-12%\n",
      "Epoch      955 Loss: 8.881784197001252e-12%\n",
      "Epoch      956 Loss: 8.881784197001252e-12%\n",
      "Epoch      957 Loss: 8.881784197001252e-12%\n",
      "Epoch      958 Loss: 8.881784197001252e-12%\n",
      "Epoch      959 Loss: 8.881784197001252e-12%\n",
      "Epoch      960 Loss: 8.881784197001252e-12%\n",
      "Epoch      961 Loss: 8.881784197001252e-12%\n",
      "Epoch      962 Loss: 8.881784197001252e-12%\n",
      "Epoch      963 Loss: 7.194245199571014e-12%\n",
      "Epoch      964 Loss: 7.194245199571014e-12%\n",
      "Epoch      965 Loss: 7.194245199571014e-12%\n",
      "Epoch      966 Loss: 7.194245199571014e-12%\n",
      "Epoch      967 Loss: 8.881784197001252e-12%\n",
      "Epoch      968 Loss: 8.881784197001252e-12%\n",
      "Epoch      969 Loss: 8.881784197001252e-12%\n",
      "Epoch      970 Loss: 7.194245199571014e-12%\n",
      "Epoch      971 Loss: 5.6843418860808015e-12%\n",
      "Epoch      972 Loss: 5.6843418860808015e-12%\n",
      "Epoch      973 Loss: 5.6843418860808015e-12%\n",
      "Epoch      974 Loss: 5.6843418860808015e-12%\n",
      "Epoch      975 Loss: 5.6843418860808015e-12%\n",
      "Epoch      976 Loss: 7.194245199571014e-12%\n",
      "Epoch      977 Loss: 7.194245199571014e-12%\n",
      "Epoch      978 Loss: 7.194245199571014e-12%\n",
      "Epoch      979 Loss: 7.194245199571014e-12%\n",
      "Epoch      980 Loss: 7.194245199571014e-12%\n",
      "Epoch      981 Loss: 7.194245199571014e-12%\n",
      "Epoch      982 Loss: 7.194245199571014e-12%\n",
      "Epoch      983 Loss: 7.194245199571014e-12%\n",
      "Epoch      984 Loss: 7.194245199571014e-12%\n",
      "Epoch      985 Loss: 7.194245199571014e-12%\n",
      "Epoch      986 Loss: 7.194245199571014e-12%\n",
      "Epoch      987 Loss: 7.194245199571014e-12%\n",
      "Epoch      988 Loss: 7.194245199571014e-12%\n",
      "Epoch      989 Loss: 7.194245199571014e-12%\n",
      "Epoch      990 Loss: 7.194245199571014e-12%\n",
      "Epoch      991 Loss: 7.194245199571014e-12%\n",
      "Epoch      992 Loss: 7.194245199571014e-12%\n",
      "Epoch      993 Loss: 7.194245199571014e-12%\n",
      "Epoch      994 Loss: 7.194245199571014e-12%\n",
      "Epoch      995 Loss: 7.194245199571014e-12%\n",
      "Epoch      996 Loss: 7.194245199571014e-12%\n",
      "Epoch      997 Loss: 7.194245199571014e-12%\n",
      "Epoch      998 Loss: 7.194245199571014e-12%\n",
      "Epoch      999 Loss: 7.194245199571014e-12%\n",
      "Epoch     1000 Loss: 7.194245199571014e-12%\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,epoch):  # trains the NN 1,000 times\n",
    "    for input, target in zip(X, y):\n",
    "        optimizer.zero_grad() \n",
    "        output = net(input)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch {: >8} Loss: {}%\".format(i+1, loss.cpu().data.numpy()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:[0,0] Target:[0] Predicted:[0.0] Error:[0.0]\n",
      "Input:[0,1] Target:[1] Predicted:[1.0] Error:[0.0]\n",
      "Input:[1,0] Target:[1] Predicted:[1.0] Error:[0.0]\n",
      "Input:[1,1] Target:[0] Predicted:[0.0] Error:[0.0]\n"
     ]
    }
   ],
   "source": [
    "for input, target in zip(X, y):\n",
    "    output = net(input)\n",
    "    print(\"Input:[{},{}] Target:[{}] Predicted:[{}] Error:[{}]\".format(\n",
    "        int(input.cpu().data.numpy()[0][0]),\n",
    "        int(input.cpu().data.numpy()[0][1]),\n",
    "        int(target.cpu().data.numpy()[0]),\n",
    "        round(float(output.cpu().data.numpy()[0]), 4),\n",
    "        round(float(abs(target.cpu().data.numpy()[0] - output.cpu().data.numpy()[0])), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchviz import make_dot, make_dot_from_trace\n",
    "#make_dot(net(X[0]), params=dict(net.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
